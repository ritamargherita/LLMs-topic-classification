{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1b66d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59a7ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b744c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education_expenditure_and_indicators\n",
    "#column_headers = [\"ID\", \"Education Sectors\", \"Periods\", \"Total Expenditure On Education\", \"Total Government\",\"Total On Education Institutions\", \"Lump Sum Financing\", \"Contract Research\", \"Operating Costs\", \"Total Subsidies And Tax Benefits\", \"Education Component Preschool Day Care\", \"Transport Of Pupils\", \"Provisions For Disabled Students\", \"Allowance For School Costs\", \"Supervising Costs Of Companies\", \"Student Grants\", \"Student Loans\", \"Total Receipts\", \"EUSubsidies\", \"Repaid Student Grants And Loans And\", \"Interest On Student Loans\", \"Repayments On Student Loans\", \"Total Expenditure By Households\", \"Total On Education Institutions\", \"Tuition Fees Subsidised Institutions\", \"Tuition Fees Private Institutions\", \"Parental Contribution School Activities\", \"Total Other Expenditures On Education\", \"Education Component Preschool Day Care\", \"Books Materials Public Transport And\", \"Wrongfully Received Student Grants And\", \"Interest On Student Loans\", \"Repayments On Student Loans\", \"Total Allowances Received\", \"Child Care Allowance Education Component\", \"Provisions For Disabled Students\", \"Allowances For School Costs\", \"Scholarships Of Companies\", \"Student Grants For School Fees Books\", \"Student Grants For The Cost Of Living\", \"Student Loans\", \"Total Companies\", \"Total On Education Institutes\", \"Supervising Trainees And Apprentices\", \"Tuition Fees Private Institutions\", \"Contract Research\", \"Total Other Expenditure On Education\", \"Child Care Allowance Education Component\", \"Scholarships Of Companies\", \"Total Subsidies Received\", \"Transport Of Pupils\", \"Supervising Costs Of Companies\", \"Total Foreign Countries\", \"Total On Education Institutes\", \"International Funds\", \"Contract Research\", \"EUSubsidies\", \"Total Expenditure On Education\", \"In Of GDP\", \"Per Capita\", \"Total Government Exp On Education\", \"In Of GDP\", \"In Of Total Government Expenditure\", \"Total On Education Institutions\", \"Total In Of GDP\", \"Government\", \"Households Companies Foreign Countries\", \"Per Student Including RD\", \"Per Student Excluding RD\"]\n",
    "\n",
    "# Health_expectancy\n",
    "#column_headers = [\"ID\", \"Sex\", \"Age At December31\", \"Type Of Figure\", \"Periods\", \"Life Expectancy\", \"Life Expectancy In Perceived Good Health\", \"LEWithout Moderate Severe Limitations\", \"LEWithout Severe Limitations\", \"LEWithout Light Moderate Severe Lim\", \"LEWithout Chronic Morbidity\", \"LEWithout Chr Morb Excl Hypertension\", \"LEWithout Psychological Complaints\", \"Life Expectancy Without GALILimitations\", \"Life Exp Without Severe GALILimitations\"]\n",
    "\n",
    "# Listed_monuments\n",
    "#column_headers = [\"ID\", \"Type Of Monument\", \"Regions\", \"Periods\", \"Listed Monuments And Historic Buildings\"]\n",
    "\n",
    "# Livestock\n",
    "#column_headers = [\"ID\", \"Farm Animals\", \"Periods\", \"Livestock\"]\n",
    "\n",
    "# Milk_supply_and_dairy_production\n",
    "#column_headers = [\"ID\", \"Periods\", \"Volume\", \"Fat Content\", \"Protein Content\", \"Butter\", \"Cheese\", \"Total Milk Powder\", \"Whole Milk Powder\", \"Skimmed Milk Powder\", \"Concentrated Milk\", \"Whey Powder\"]\n",
    "\n",
    "# Mobility\n",
    "column_headers = [\"ID\", \"Travel Motives\", \"Population\", \"Travel Modes\", \"Margins\", \"Region Characteristics\", \"Periods\", \"Trips\", \"Distance Travelled\", \"Time Travelled\", \"Trips\", \"Distance Travelled\", \"Time Travelled\"]\n",
    "\n",
    "# Plant_protection_products\n",
    "#column_headers = [\"ID\", \"Plant Protection Products\", \"Periods\", \"Sales Active Ingredient\", \"Sales Active Ingredient Indexnumber\"]\n",
    "\n",
    "# Population_dynamics\n",
    "#column_headers = [\"ID\", \"Periods\", \"Population At Start Of Selected Period\", \"Live Births\", \"Deaths\", \"Immigration\", \"Emigration Including Administrative C\", \"Net Corrections\", \"Total Population Growth\", \"Population At End Of Selected Period\"]\n",
    "\n",
    "# Social_security\n",
    "#column_headers = [\"ID\", \"Periods\", \"Total Number Of Disablement Benefits\", \"Benefits WAO\", \"Benefits Wajong\", \"Benefits WAZ\", \"Total Benefits WIA\", \"Benefits IVA\", \"Benefits WGA\", \"Benefits WWNot Seasonally Adjusted\", \"Benefits WWSeasonally Adjusted\", \"Benefits IOW\", \"Total Benefits Income Support\", \"Benefits Income Support Up To AOWAge\", \"Benefits Income Support From AOWAge\", \"Benefits IOAW\", \"Benefits IOAZ\", \"Benefits AOW\", \"Benefits Anw\", \"Persons Entitled To AKW\"]\n",
    "\n",
    "# Trade_and_industry\n",
    "#column_headers = [\"ID\", \"Sector Branches SIC2008\", \"Periods\", \"Employee\", \"Employed Person\", \"Employee\", \"Employed Person\", \"Total\", \"Net Turnover\", \"Other Revenues\", \"Total\", \"Total\", \"Total\", \"Initial Stock Of Raw And Auxiliary Mater\", \"Purchased Raw And Auxiliary Materials\", \"Closing Stock Raw And Auxiliary Mat\", \"Total\", \"Initial Stock Of Commodities\", \"Purchased Commodities\", \"Closing Stock Of Commodities\", \"Payments To Subcontractors\", \"Purchase Value Not Elsewhere Classified\", \"Total\", \"Gross Wages And Salaries\", \"Total\", \"Employer Paid Social Insurance Premiums\", \"Other Social Insurance Costs\", \"Pension And Early Retirement Premiums\", \"Other Personnel Costs\", \"Total\", \"Costs Of Energy Use\", \"Housing Costs\", \"Equipment And Inventory Costs\", \"Transport Costs\", \"Sales Costs\", \"Communication Costs\", \"Costs Of Other Services\", \"Other Liabilities\", \"Depreciation Fixed Assets\", \"Operating Results\", \"Net Financial Income\",\"Balance Of Provisions\", \"Net Extraordinary Income\"\"Pre Tax Results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db115367",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [{\"label\": \"Demography\", \"description\": \"Vital Statistics\"},\n",
    "          {\"label\": \"Topic2\", \"description\": \"Description of Topic2\"},\n",
    "          {\"label\": \"Topic3\", \"description\": \"Description of Topic3\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a70d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b126d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b0c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5fa3e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "326fb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "074ff237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "words = \"travel modes\"\n",
    "text = \"travel modes, population, margins\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5a55ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_words = \"[CLS] \" + words + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa18d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_words = tokenizer.tokenize(marked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dddaa48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "travel        3,604\n",
      "modes        11,583\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens_words = tokenizer.convert_tokens_to_ids(tokenized_words)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_words, indexed_tokens_words):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bcbfb3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids_words = [1] * len(tokenized_words)\n",
    "\n",
    "print (segments_ids_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f4100ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor_words = torch.tensor([indexed_tokens_words])\n",
    "segments_tensors_words = torch.tensor([segments_ids_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "168f78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "63642e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor_words, segments_tensors_words)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states_words = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a24f1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs_words = hidden_states_words[-2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fefd894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embedding = torch.mean(token_vecs_words, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103f4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305cece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "91425f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c5ad8372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "travel        3,604\n",
      "modes        11,583\n",
      ",             1,010\n",
      "population    2,313\n",
      ",             1,010\n",
      "margins      17,034\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eb68d8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0e3affaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "33b04ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "22a685c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "96bb38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b49f10d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 8 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1d58cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 travel\n",
      "2 modes\n",
      "3 ,\n",
      "4 population\n",
      "5 ,\n",
      "6 margins\n",
      "7 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7577844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 travel\n",
      "2 modes\n",
      "3 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_words):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "80b7e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5937124",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cosine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mcosine\u001b[49m(token_vecs_sum[\u001b[38;5;241m10\u001b[39m], token_vecs_sum[\u001b[38;5;241m19\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cosine' is not defined"
     ]
    }
   ],
   "source": [
    "diff = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d781b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
